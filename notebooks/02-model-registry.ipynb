{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow: model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.compose as compose\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.model_selection as model_selection \n",
    "import sklearn.metrics as metrics\n",
    "import mlflow\n",
    "import mlflow.tracking as tracking\n",
    "import mlflow.sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_EXPERIMENT = \"Model registy demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Attrition\", axis=1)\n",
    "y = df.loc[:, \"Attrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "for colname, coltype in X.dtypes.items():\n",
    "    if coltype == \"object\":\n",
    "        categorical_cols.append(colname)\n",
    "    # In our data there are only 2 types\n",
    "    else:\n",
    "        numerical_cols.append(colname)\n",
    "    \n",
    "print(f\"Categorical: {', '.join(categorical_cols)}\")\n",
    "print(f\"Numerical: {', '.join(numerical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "oh_encoder = preprocessing.OneHotEncoder(categories=\"auto\", drop=\"first\")\n",
    "\n",
    "preprocess_pipeline = compose.ColumnTransformer(transformers=[ # (name, transformer, column(s))\n",
    "    (\"scaler\", scaler, numerical_cols),\n",
    "    (\"one_hot_encode\", oh_encoder, categorical_cols)\n",
    "], remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_encoded_categorical_colnames = preprocess_pipeline.named_transformers_[\"one_hot_encode\"].get_feature_names().tolist()\n",
    "\n",
    "transformed_colnames = numerical_cols + oh_encoded_categorical_colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make target binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda y: 1 if y == \"Yes\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up MLflow experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if an experiment already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tracking.MlflowClient()\n",
    "experiments = client.list_experiments()\n",
    "if MLFLOW_EXPERIMENT in [e.name for e in experiments]:\n",
    "    print(\"Experiment already exists.\")\n",
    "else:\n",
    "    print(\"Experiment does not exist. Creating it.\")\n",
    "    mlflow.create_experiment(MLFLOW_EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "l1_ratio = 0.5\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Define model\n",
    "    model = linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    # Train a model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Compute CV scores\n",
    "    scores = model_selection.cross_validate(\n",
    "        model, \n",
    "        X, \n",
    "        y, \n",
    "        cv=5, \n",
    "        scoring={\n",
    "            \"roc_auc\": metrics.make_scorer(metrics.roc_auc_score),\n",
    "            \"log_loss\": metrics.make_scorer(metrics.log_loss)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log paramas\n",
    "    mlflow.log_params({\n",
    "        \"alpha\": alpha,\n",
    "        \"l1_ratio\": l1_ratio\n",
    "    })            \n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"val_roc_auc\": scores[\"test_roc_auc\"].mean(),\n",
    "        \"val_log_loss\": scores[\"test_log_loss\"].mean(),\n",
    "    })\n",
    "\n",
    "    # Log model tags, e.g., model type\n",
    "    mlflow.set_tags({\n",
    "        \"model\": \"ElasticNet\"\n",
    "    })\n",
    "\n",
    "    # Log model itself\n",
    "    mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    # Log preprocessing pipeline\n",
    "    mlflow.sklearn.log_model(preprocess_pipeline, artifact_path=\"preprocess_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to MLflow and register both the pipeline and the model as `Pipeline` and `Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load registered model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/processed/test.csv\")\n",
    "X_test = df_test.drop(\"Attrition\", axis=1)\n",
    "y_test = df_test.loc[:, \"Attrition\"]\n",
    "y_test = y_test.apply(lambda y: 1 if y == \"Yes\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, preprocess_pipeline  # just showing that it's not in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_uri = client.get_model_version_details(name=\"Pipeline\", version=1).source\n",
    "preprocess_pipeline = mlflow.sklearn.load_model(pipeline_uri)\n",
    "\n",
    "model_uri = client.get_model_version_details(name=\"Model\", version=1).source\n",
    "model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(preprocess_pipeline.transform(X_test))\n",
    "print(f\"ROC AUC:  {metrics.roc_auc_score(y_test, y_hat):.3f}\")\n",
    "print(f\"Log loss: {metrics.log_loss(y_test, y_hat):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "l1_ratio = 0.3\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Define model\n",
    "    model = linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    # Train a model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Compute CV scores\n",
    "    scores = model_selection.cross_validate(\n",
    "        model, \n",
    "        X, \n",
    "        y, \n",
    "        cv=5, \n",
    "        scoring={\n",
    "            \"roc_auc\": metrics.make_scorer(metrics.roc_auc_score),\n",
    "            \"log_loss\": metrics.make_scorer(metrics.log_loss)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log paramas\n",
    "    mlflow.log_params({\n",
    "        \"alpha\": alpha,\n",
    "        \"l1_ratio\": l1_ratio\n",
    "    })            \n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"val_roc_auc\": scores[\"test_roc_auc\"].mean(),\n",
    "        \"val_log_loss\": scores[\"test_log_loss\"].mean(),\n",
    "    })\n",
    "\n",
    "    # Log model tags, e.g., model type\n",
    "    mlflow.set_tags({\n",
    "        \"model\": \"ElasticNet\"\n",
    "    })\n",
    "\n",
    "    # Log model itself\n",
    "    mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    # Log preprocessing pipeline\n",
    "    mlflow.sklearn.log_model(preprocess_pipeline, artifact_path=\"preprocess_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go and register a new model (the pipeline is the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load registered model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, preprocess_pipeline  # just showing that it's not in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that it is version 2 for the model now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_uri = client.get_model_version_details(name=\"Pipeline\", version=1).source\n",
    "preprocess_pipeline = mlflow.sklearn.load_model(pipeline_uri)\n",
    "\n",
    "model_uri = client.get_model_version_details(name=\"Model\", version=2).source\n",
    "model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(preprocess_pipeline.transform(X_test))\n",
    "print(f\"ROC AUC:  {metrics.roc_auc_score(y_test, y_hat):.3f}\")\n",
    "print(f\"Log loss: {metrics.log_loss(y_test, y_hat):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
